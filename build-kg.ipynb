{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "We want to create a minified knowledge graph that can be generated quickly and economically from existing data.\n",
    "\n",
    "We will do so via the following process, an iterative method of constructing knowledge graphs from unstructured, unlabeled text data.\n",
    "\n",
    "* Define an empty set of unique entities $E$.\n",
    "* Define an empty set of unique properties $P$.\n",
    "* Define an empty knowledge graph $G$ as a set of 3-tuples $(e_1, p, e_2)$ s.t. $e_1, e_2\\in E$ and $p\\in P$.\n",
    "* Preprocess the input text data. We only applied minimal preprocessing, with the aim of retaining as much information as possible from the original data. In fact, we augment the existing text data by providing simple *coreference resolution*, in the hope of restoring information that would otherwise be lost by sentence-level chunking.\n",
    "    * For example: \"Barack Obama was the 44th President of the United States. *He* was nominated as the Democratic Party's candidate in 2008.\" $\\to$ \"... *Barack Obama* was nominated...\" \n",
    "* Chunk the preprocessed text dataset into a size appropriate for processing within an LLM's context window; we found 1 sentence to be optimal.\n",
    "* For each data chunk $d$, do the following:\n",
    "  * Pass $E$, $P$, and $d$ to the LLM. Prompt it to return $C$, a set of 3-tuples $(e_1, p, e_2)$ s.t. $e_1, e_2\\in E$ and $p\\in P$. The instructions should indicate that, when creating each element $c$ of $C$, if an entity or property referenced in $c$ is not present in $E$ or $P$, it should define a new entity/property and indicate that it is new.\n",
    "  * For each claim $c\\in C$, if either of $e_1, e_2\\notin E$ or $p\\notin P$, add the relevant element to its set. Then, append $c$ to the locally-constructed knowledge graph $G$.\n",
    "\n",
    "In this way, we iteratively build a knowledge graph using only the entity and property types that are relevant to the data in our input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mwxml in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (0.3.6)\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dotenv in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: openai in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (1.77.0)\n",
      "Requirement already satisfied: pydantic in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (2.11.4)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from mwxml) (4.23.0)\n",
      "Requirement already satisfied: mwcli>=0.0.2 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from mwxml) (0.0.3)\n",
      "Requirement already satisfied: mwtypes>=0.4.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from mwxml) (0.4.0)\n",
      "Requirement already satisfied: para>=0.0.1 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from mwxml) (0.0.8)\n",
      "Requirement already satisfied: filelock in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from jsonschema>=2.5.1->mwxml) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from jsonschema>=2.5.1->mwxml) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from jsonschema>=2.5.1->mwxml) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from jsonschema>=2.5.1->mwxml) (0.24.0)\n",
      "Requirement already satisfied: docopt in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from mwcli>=0.0.2->mwxml) (0.6.2)\n",
      "Requirement already satisfied: jsonable>=0.3.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from mwtypes>=0.4.0->mwxml) (0.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence-transformers in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mwxml numpy==1.26.4 torch torchvision torchaudio python-dotenv openai pydantic\n",
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import userdata\n",
    "  openai_token = userdata.get(\"OPENAI_API_KEY\")\n",
    "else:\n",
    "  import os\n",
    "  import dotenv\n",
    "  dotenv.load_dotenv()\n",
    "  openai_token = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "assert openai_token is not None, \"Must set the OPENAI_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"proof-of-concept/unique_wiki_urls.txt\", \"r\") as f:\n",
    "    wiki_urls = f.read().split(\"\\n\")\n",
    "\n",
    "wiki_urls = sorted([\n",
    "    url.replace(\"_\", \" \").replace(\"-COLON-\", \":\")\n",
    "    for url in wiki_urls\n",
    "    if url\n",
    "])\n",
    "len(wiki_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1460/1460 [04:05<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "BASE_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "summaries = {}\n",
    "for slug in tqdm(wiki_urls):\n",
    "    try:\n",
    "        data = requests.get(\n",
    "            BASE_URL,\n",
    "            params={\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"titles\": slug,\n",
    "                \"prop\": \"extracts\",\n",
    "                \"exintro\": True,\n",
    "                \"explaintext\": True\n",
    "            }\n",
    "        ).json()\n",
    "        summary = next(iter(data[\"query\"][\"pages\"].values()))[\"extract\"]\n",
    "        summaries[slug] = summary\n",
    "    except Exception as e:\n",
    "        print(slug, e)\n",
    "        continue\n",
    "\n",
    "with open(\"all_summaries.json\", \"w\") as f:\n",
    "    json.dump(summaries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Lightly preprocess summaries. In particular, we naively replace all\n",
    "#  third-person personal pronouns (he/she/they/them/it) with the\n",
    "#  title of the article.\n",
    "\n",
    "with open(\"all_summaries.json\", \"r\") as f:\n",
    "    summaries = json.load(f)\n",
    "\n",
    "summaries = {\n",
    "    title: re.sub(r\"\\s+([hH]e|[sS]he|[tT]hey|[tT]hem|[iI]t)[\\s\\.,!?]+\", f\" {title} \", summary).replace(\"...\", \".\")\n",
    "    for title, summary in summaries.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from typing import List\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "class SemanticTriple(BaseModel):\n",
    "  entityA: str\n",
    "  relationship: str\n",
    "  entityB: str\n",
    "    \n",
    "  def __hash__(self):\n",
    "      return hash((self.entityA, self.relationship, self.entityB))\n",
    "      \n",
    "class SemanticTripleList(BaseModel):\n",
    "    triples: List[SemanticTriple]\n",
    "\n",
    "# def create_extract_triples_fn(entities, relations):\n",
    "#     return {\n",
    "#         \"name\": \"extract_triples\",\n",
    "#         \"description\": f\"\"\"\n",
    "#         Extract all semantic triples (entityA, relationship, entityB) from a sentence.\n",
    "#         Attempt to do so using only the entities and relationships provided to you, to the\n",
    "#         best of your ability.\n",
    "                \n",
    "#         Return a JSON object with a single field `triples`, an array of objects:\n",
    "#           {{ \"new\": <true|false>, \"entityA\": <ENTITY_ID>, \"relationship\": <RELATION_ID>, \"entityB\": <ENTITY_ID> }}\n",
    "        \n",
    "#         If there are no triples, return `{{\"triples\":[]}}`.\n",
    "#         \"\"\",\n",
    "#         \"parameters\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"triples\": {\n",
    "#                     \"type\": \"array\",\n",
    "#                     \"items\": {\n",
    "#                         \"type\": \"object\",\n",
    "#                         \"properties\": {\n",
    "#                             \"entityA\": {\n",
    "#                                 \"type\": \"string\",\n",
    "#                                 \"enum\": entities,\n",
    "#                                 \"description\": \"ID of the first entity\"\n",
    "#                             },\n",
    "#                             \"relationship\": {\n",
    "#                                 \"type\": \"string\",\n",
    "#                                 \"enum\": relations,\n",
    "#                                 \"description\": \"ID of the relationship\"\n",
    "#                             },\n",
    "#                             \"entityB\": {\n",
    "#                                 \"type\": \"string\",\n",
    "#                                 \"enum\": entities,\n",
    "#                                 \"description\": \"ID of the second entity\"\n",
    "#                             },\n",
    "#                         },\n",
    "#                         \"required\": [\"entityA\", \"relationship\", \"entityB\"],\n",
    "#                     },\n",
    "#                 },\n",
    "#             },\n",
    "#             \"required\": [\"triples\"],\n",
    "#         },\n",
    "#     }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SemanticTripleExtractor:\n",
    "    client: openai.OpenAI\n",
    "    GPT_MODEL = \"gpt-4o\"\n",
    "    ERROR_RETRY_SLEEP = 0.001\n",
    "\n",
    "    def get_semantic_triples(self, text: str):\n",
    "        system_prompt = \"\"\"\n",
    "        You are a semantic role and entity extractor.\n",
    "\n",
    "        Given an input text (which may contain multiple sentences), identify every (entityA, relationship, entityB) tuple,\n",
    "        **even if it's factually incorrect**.\n",
    "\n",
    "        Some sentences may contain multiple triples, and the semantic triples that are explicitly stated in the sentence\n",
    "        may not be the only implications of the sentence. For example, the sentence \"John graduated college\" also implies\n",
    "        the sentence \"John holds a degree\". Within reason, attempt to capture all explicit and implicit semantic triples.\n",
    "\n",
    "        Always output exactly valid JSON with a single key \"triples\" consisting of a list of semantic triples:\n",
    "        {\n",
    "        \"triples\": [\n",
    "            { \"entityA\": \"<ENTITY_ID>\", \"relationship\": \"<REL_ID>\", \"entityB\": \"<ENTITY_ID>\" },\n",
    "            …\n",
    "        ]\n",
    "        }\n",
    "        If there are none, return `{ \"triples\": [] }`.\n",
    "        All relationships should be formatted using camelCase, and all entities should use PascalCase.\n",
    "        ---\n",
    "        Below is an example of proper processing.\n",
    "        Sentence: \"Princess Diana is a British royal.\"\n",
    "        Output: {\n",
    "            [\"entityA\": \"PrincessDiana\", \"relationship\": \"countryOfOrigin\", \"entityB\": \"GreatBritain\"],\n",
    "            [\"entityB\": \"PrincessDiana\", \"relationship\": \"instanceOf\", \"entityB\": \"Royal\"]\n",
    "        }\n",
    "        ---\n",
    "        Below is another example of proper processing.\n",
    "        Sentence: \"Batman Forever was released on June 16, 1995, to mixed reviews from critics, who praised the visuals, action sequences, and soundtrack, but criticized the screenplay and tonal departure from previous two films.\"\n",
    "        Output: {\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"releaseDate\", \"entityB\": \"June16,1995\"],\n",
    "            [\"entityB\": \"BatmanForever\", \"relationship\": \"receivedReviews\", \"entityB\": \"Mixed\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"praisedFor\", \"entityB\": \"Visuals\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"praisedFor\", \"entityB\": \"ActionSequences\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"praisedFor\", \"entityB\": \"Soundtrack\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"criticizedFor\", \"entityB\": \"Screenplay\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"criticizedFor\", \"entityB\": \"TonalDepartureFromPreviousFilms\"]\n",
    "        }\n",
    "        ---\n",
    "        Think step by step before giving your output.\n",
    "        \"\"\"\n",
    "        return self._request_with_retry(system_prompt, text)\n",
    "\n",
    "    def _request_with_retry(self, system_prompt: str, text: str):\n",
    "        n_retries = 0\n",
    "        while True:\n",
    "            try:\n",
    "                response = (\n",
    "                    self.client.beta.chat.completions.parse(\n",
    "                        model=self.GPT_MODEL,\n",
    "                        temperature=0,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": text},\n",
    "                        ],\n",
    "                        response_format=SemanticTripleList,\n",
    "                    )\n",
    "                ).choices[0].message\n",
    "                break\n",
    "\n",
    "            except openai.RateLimitError as err:\n",
    "                n_retries += 1\n",
    "                print(err)\n",
    "                print(\"Exceeded rate limit\")\n",
    "                print(f\"Sleeping before retry (done {n_retries} time(s))\")\n",
    "                time.sleep(self.ERROR_RETRY_SLEEP)\n",
    "\n",
    "            except Exception as err:\n",
    "                n_retries += 1\n",
    "                print(f\"Unexpected error ({err})\")\n",
    "                print(f\"Sleeping before retry (done {n_retries} time(s))\")\n",
    "                time.sleep(self.ERROR_RETRY_SLEEP)\n",
    "\n",
    "        if response is None:\n",
    "            raise ValueError(\"Got null response\")\n",
    "        elif response.refusal:\n",
    "            raise ValueError(response.refusal)\n",
    "        \n",
    "        #data = json.loads(response.arguments)[\"triples\"]\n",
    "        #adapter = TypeAdapter(list[SemanticTriple])\n",
    "\n",
    "        #return response.validate_python(data)\n",
    "        return SemanticTripleList.model_validate_json(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=openai_token)\n",
    "semantic_extractor = SemanticTripleExtractor(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triples = semantic_extractor.get_semantic_triples(\"Val Kilmer played Batman in Batman Forever.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entityA='ValKilmer' relationship='playedCharacter' entityB='Batman'\n",
      "entityA='ValKilmer' relationship='actedIn' entityB='BatmanForever'\n",
      "entityA='Batman' relationship='characterIn' entityB='BatmanForever'\n"
     ]
    }
   ],
   "source": [
    "for i in test_triples.triples:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohankashyap/opt/anaconda3/envs/py12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "played character\n",
      "tensor([[0.1686]])\n",
      "film in series\n",
      "tensor([[0.1779]])\n",
      "installment number\n",
      "tensor([[0.2224]])\n",
      "is sequel to\n",
      "tensor([[0.1611]])\n",
      "film release date\n",
      "tensor([[0.1164]])\n",
      "received reviews\n",
      "tensor([[0.4093]])\n",
      "praised\n",
      "tensor([[0.6769]])\n",
      "praised\n",
      "tensor([[0.6769]])\n",
      "praised\n",
      "tensor([[0.6769]])\n",
      "criticized\n",
      "tensor([[0.4865]])\n",
      "criticized\n",
      "tensor([[0.4865]])\n",
      "from films\n",
      "tensor([[0.2182]])\n"
     ]
    }
   ],
   "source": [
    "for i in test_triples.triples:\n",
    "    print(re.sub(\"[A-Z]\", lambda m: \" \"+m.group(0).lower(), i.relationship))\n",
    "    encoding = model.encode(re.sub(\"[A-Z]\", lambda m: \" \"+m.group(0).lower(), i.relationship))\n",
    "    print(model.similarity(encoding,model.encode(\"complimented\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17186"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks = []\n",
    "for title, summary in summaries.items():\n",
    "    all_chunks += [\n",
    "        f\"(Topic: {title}) {sentence}\"\n",
    "        for sentence in re.split(r\"[\\.!?]\", summary)\n",
    "    ]\n",
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "all_entities = []\n",
    "all_properties = []\n",
    "knowledge_graph = []\n",
    "\n",
    "chunk_sample = random.sample(all_chunks, 5)\n",
    "\n",
    "for chunk in tqdm(chunk_sample):\n",
    "    triples = semantic_extractor.get_semantic_triples(chunk)\n",
    "    for triple in triples.triples:\n",
    "        re.sub(\"[A-Z]\", lambda m: \" \"+m.group(0).lower(), triple.relationship)\n",
    "        if triple.entityA not in all_entities:\n",
    "            all_entities.append(triple.entityA)\n",
    "        if triple.entityB not in all_entities:\n",
    "            all_entities.append(triple.entityB)\n",
    "        if triple.relationship not in all_properties:\n",
    "            all_properties.append(triple.relationship)\n",
    "    knowledge_graph += [t.model_dump() for t in triples.triples]\n",
    "\n",
    "with open(\"extracted-kg.json\", \"w\") as f:\n",
    "    json.dump(knowledge_graph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BretEastonEllis', 'LiteraryBratPack', 'Satirist', 'ExpressionOfExtremeActsAndOpinionsInAnAffectlessStyle', 'ColumbiaPictures', 'FilmStudio', 'World', 'LittleThree', 'MajorFilmStudios', 'Hollywood', 'TaranHourieKillam', 'April11982', 'American', 'Actor', 'Comedian', 'TenaciousD', 'DavidCross', 'JackBlack', 'MrShow', 'Series', 'MeetingOfTenaciousDAndDavidCross', 'AlanJPakula', 'ComesAHorseman', 'StartingOver', 'SophiesChoice', 'PresumedInnocent', 'ThePelicanBrief', '1978', '1979', '1982', '1990', '1993']\n"
     ]
    }
   ],
   "source": [
    "print(all_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
