{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import userdata\n",
    "  openai_token = userdata.get(\"OPENAI_API_KEY\")\n",
    "else:\n",
    "  import os\n",
    "  import dotenv\n",
    "  dotenv.load_dotenv()\n",
    "  openai_token = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "assert openai_token is not None, \"Must set the OPENAI_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class SemanticTriple(BaseModel):\n",
    "  entityA: str\n",
    "  relationship: str\n",
    "  entityB: str\n",
    "    \n",
    "  def __hash__(self):\n",
    "      return hash((self.entityA, self.relationship, self.entityB))\n",
    "      \n",
    "class SemanticTripleList(BaseModel):\n",
    "    triples: List[SemanticTriple]\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class SemanticTripleExtractor:\n",
    "    client: openai.OpenAI\n",
    "    GPT_MODEL = \"gpt-4o-mini\"\n",
    "    ERROR_RETRY_SLEEP = 0.001\n",
    "\n",
    "    def get_semantic_triples(self, text: str):\n",
    "        system_prompt = \"\"\"\n",
    "        You are a semantic role and entity extractor.\n",
    "\n",
    "        Given an input text (which may contain multiple sentences), identify every (entityA, relationship, entityB) tuple,\n",
    "        **even if it's factually incorrect**.\n",
    "\n",
    "        Some sentences may contain multiple triples, and the semantic triples that are explicitly stated in the sentence\n",
    "        may not be the only implications of the sentence. For example, the sentence \"John graduated college\" also implies\n",
    "        the sentence \"John holds a degree\". Within reason, attempt to capture all explicit and implicit semantic triples.\n",
    "\n",
    "        Always output exactly valid JSON with a single key \"triples\" consisting of a list of semantic triples:\n",
    "        {\n",
    "        \"triples\": [\n",
    "            { \"entityA\": \"<ENTITY_ID>\", \"relationship\": \"<REL_ID>\", \"entityB\": \"<ENTITY_ID>\" },\n",
    "            …\n",
    "        ]\n",
    "        }\n",
    "        If there are none, return `{ \"triples\": [] }`.\n",
    "        All relationships should be formatted using camelCase, and all entities should use PascalCase.\n",
    "        ---\n",
    "        Below is an example of proper processing.\n",
    "        Sentence: \"Princess Diana is a British royal.\"\n",
    "        Output: {\n",
    "            [\"entityA\": \"PrincessDiana\", \"relationship\": \"countryOfOrigin\", \"entityB\": \"GreatBritain\"],\n",
    "            [\"entityB\": \"PrincessDiana\", \"relationship\": \"instanceOf\", \"entityB\": \"Royal\"]\n",
    "        }\n",
    "        ---\n",
    "        Below is another example of proper processing.\n",
    "        Sentence: \"Batman Forever was released on June 16, 1995, to mixed reviews from critics, who praised the visuals, action sequences, and soundtrack, but criticized the screenplay and tonal departure from previous two films.\"\n",
    "        Output: {\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"releaseDate\", \"entityB\": \"June16,1995\"],\n",
    "            [\"entityB\": \"BatmanForever\", \"relationship\": \"receivedReviews\", \"entityB\": \"Mixed\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"praisedFor\", \"entityB\": \"Visuals\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"praisedFor\", \"entityB\": \"ActionSequences\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"praisedFor\", \"entityB\": \"Soundtrack\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"criticizedFor\", \"entityB\": \"Screenplay\"],\n",
    "            [\"entityA\": \"BatmanForever\", \"relationship\": \"criticizedFor\", \"entityB\": \"TonalDepartureFromPreviousFilms\"]\n",
    "        }\n",
    "        ---\n",
    "        Think step by step before giving your output.\n",
    "        \"\"\"\n",
    "        return self._request_with_retry(system_prompt, text)\n",
    "\n",
    "    def _request_with_retry(self, system_prompt: str, text: str):\n",
    "        n_retries = 0\n",
    "        while True:\n",
    "            try:\n",
    "                response = (\n",
    "                    self.client.beta.chat.completions.parse(\n",
    "                        model=self.GPT_MODEL,\n",
    "                        temperature=0,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": text},\n",
    "                        ],\n",
    "                        response_format=SemanticTripleList,\n",
    "                    )\n",
    "                ).choices[0].message\n",
    "                break\n",
    "\n",
    "            except openai.RateLimitError as err:\n",
    "                n_retries += 1\n",
    "                print(err)\n",
    "                print(\"Exceeded rate limit\")\n",
    "                print(f\"Sleeping before retry (done {n_retries} time(s))\")\n",
    "                time.sleep(self.ERROR_RETRY_SLEEP)\n",
    "\n",
    "            except Exception as err:\n",
    "                n_retries += 1\n",
    "                print(f\"Unexpected error ({err})\")\n",
    "                print(f\"Sleeping before retry (done {n_retries} time(s))\")\n",
    "                time.sleep(self.ERROR_RETRY_SLEEP)\n",
    "\n",
    "        if response is None:\n",
    "            raise ValueError(\"Got null response\")\n",
    "        elif response.refusal:\n",
    "            raise ValueError(response.refusal)\n",
    "        \n",
    "        return SemanticTripleList.model_validate_json(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class BoolQResponse(BaseModel):\n",
    "    answer: bool\n",
    "    reason: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClaimEvaluator:\n",
    "    client: openai.OpenAI\n",
    "    GPT_MODEL = \"gpt-4o-mini\"\n",
    "    ERROR_RETRY_SLEEP = 0.001\n",
    "\n",
    "    def evaluate(self, claim: str, facts: list[str]):\n",
    "        system_prompt = \"\"\"\n",
    "        You are a fact-checking assistant.  You will be given:\n",
    "\n",
    "        • A single claim.\n",
    "        • A yes-or-no question.\n",
    "\n",
    "        Your job is to decide, using ONLY the provided facts and no external or background knowledge,\n",
    "        whether the answer to the question is true or false. Return only your true/false answer and a short explanation.\n",
    "        \"\"\"\n",
    "        return self._request_with_retry(system_prompt, claim, facts)\n",
    "\n",
    "    def _request_with_retry(self, system_prompt: str, claim: str, facts: list[str]):\n",
    "        facts_block = \"\\n\".join(f\"- {fact}\" for fact in facts)\n",
    "        user_content = f\"Claim:\\n{claim}\\n\\nFacts:\\n{facts_block}\"\n",
    "        \n",
    "        n_retries = 0\n",
    "        while True:\n",
    "            try:\n",
    "                response = (\n",
    "                    self.client.beta.chat.completions.parse(\n",
    "                        model=self.GPT_MODEL,\n",
    "                        temperature=0,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_content},\n",
    "                        ],\n",
    "                        response_format=BoolQResponse,\n",
    "                    )\n",
    "                ).choices[0].message\n",
    "                break\n",
    "\n",
    "            except openai.RateLimitError as err:\n",
    "                n_retries += 1\n",
    "                print(err)\n",
    "                print(\"Exceeded rate limit\")\n",
    "                print(f\"Sleeping before retry (done {n_retries} time(s))\")\n",
    "                time.sleep(self.ERROR_RETRY_SLEEP)\n",
    "\n",
    "            except Exception as err:\n",
    "                n_retries += 1\n",
    "                print(f\"Unexpected error ({err})\")\n",
    "                print(f\"Sleeping before retry (done {n_retries} time(s))\")\n",
    "                time.sleep(self.ERROR_RETRY_SLEEP)\n",
    "\n",
    "        if response is None:\n",
    "            raise ValueError(\"Got null response\")\n",
    "        elif response.refusal:\n",
    "            raise ValueError(response.refusal)\n",
    "        \n",
    "        return BoolQResponse.model_validate_json(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our pipeline for question evaluation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "client = openai.OpenAI(api_key=openai_token)\n",
    "semantic_extractor = SemanticTripleExtractor(client)\n",
    "evaluator = ClaimEvaluator(client)\n",
    "\n",
    "\n",
    "def evaluate_claim(passage: str, claim: str) -> BoolQResponse:\n",
    "    facts = np.concatenate([\n",
    "        semantic_extractor.get_semantic_triples(sentence).triples\n",
    "        for sentence in passage.split(\".\")\n",
    "    ]).tolist()\n",
    "    # Lexicalize each extracted fact\n",
    "    fact_strings = [\n",
    "        \" \".join([\n",
    "            f.entityA,\n",
    "            f.relationship,\n",
    "            f.entityB,\n",
    "        ])\n",
    "        for f in facts\n",
    "    ]\n",
    "    \n",
    "    # Ask arbiter to determine answer to question based on facts\n",
    "    response = evaluator.evaluate(claim, fact_strings)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'passage'],\n",
       "        num_rows: 9427\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'passage'],\n",
       "        num_rows: 3270\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "boolq = load_dataset(\"google/boolq\")\n",
    "boolq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False, True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boolean values\n",
    "set(boolq[\"train\"][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 126/250 [25:48<23:04, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error (Error code: 500 - {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}})\n",
      "Sleeping before retry (done 1 time(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [55:04<00:00, 13.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "answer_df = pd.read_csv(\"model_output.csv\")\n",
    "\n",
    "boolq_test = boolq[\"train\"].to_list() + boolq[\"validation\"].to_list()\n",
    "boolq_test = pd.DataFrame(boolq_test)\n",
    "# Remove already-processed queries\n",
    "boolq_test = boolq_test[~boolq_test[\"question\"].isin(answer_df[\"question\"])]\n",
    "boolq_test = boolq_test.sample(250)\n",
    "\n",
    "answers = []\n",
    "\n",
    "for idx, entry in tqdm(boolq_test.iterrows(), total=250):\n",
    "    true_labels.append(entry[\"answer\"])\n",
    "    eval = evaluate_claim(entry[\"passage\"], entry[\"question\"])\n",
    "    pred_labels.append(eval.answer)\n",
    "    answers.append({\n",
    "        \"true_label\": entry[\"answer\"],\n",
    "        \"pred_label\": eval.answer,\n",
    "        \"passage\": entry[\"passage\"],\n",
    "        \"question\": entry[\"question\"],\n",
    "        \"model_reason\": eval.reason,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.concat([answer_df, pd.DataFrame(answers)])\n",
    "output_df.to_csv(\"model_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.89      0.84       404\n",
      "        True       0.92      0.84      0.88       596\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.86      0.87      0.86      1000\n",
      "weighted avg       0.87      0.86      0.86      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "scores_df = pd.read_csv(\"model_output.csv\")\n",
    "scores_df = scores_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
    "\n",
    "report = classification_report(\n",
    "    scores_df[\"true_label\"], scores_df[\"pred_label\"]\n",
    ")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
