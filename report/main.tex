% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Automated Fact-Checking using Semantic triple Extraction and Knowledge Graphs}

\author{
  Rohan Kashyap \\
  \texttt{rkashyap@unc.edu} \\\And
  Nolan Welch \\
  \texttt{nwelch@unc.edu} \\\And
  Jiahao Tang \\
  \texttt{jtang@unc.edu}
}

\begin{document}
\maketitle
\begin{abstract}
Automated fact-checking pipelines has grown in importance over the years because of the increase in online presence of everyone and everything. In this research, an automated pipeline for fact-checking given text was developed and evaluated for performance. Factual information was extracted from selective articles on Wikipedia, formatted into a local knowledge graph, and used to compare and validate the factuality of other given claims. The results suggested that the proposed pipeline was indeed able to characterize the factuality of given claims ("to fact-check") based on the knowledge graph, and did so with quality. It is likely that the pipeline can be deployed for practical uses after further expanding the knowledge graph to include more entries; this pipeline should serve as a great tool in assistance to fact-checking in general. 
\end{abstract}

\section{Introduction}
Automated fact-checking (AFC) is a relatively recent development in the field of natural language processing (NLP), driven by the increasing volume of misinformation in digital media and the need for scalable verification methods. While early AFC systems primarily relied on rule-based approaches, recent advancements have seen the rise of purely neural architectures that aim to predict the truthfulness of claims based on large-scale language models. These systems have demonstrated promising results in benchmarks such as FEVER \cite{fever}, leveraging deep learning to generalize across diverse claim-evidence pairs.

However, despite previous success, purely neural models present several challenges:
\begin{itemize}
    \item Lack of interpretability: In AFC, it is particularly important to have the ability to pinpoint the relevant source material(s), as reference, when a classification of truth/false is made. Neural networks often behave as “black boxes” and provide little to no support for such demands. 
    \item Susceptibility to bias: due to the nature of neural models, they are prone to parroting arbitrary correlations present in the training data, some of which may be completely baseless. This property of neural models can lead to unreliable classifications of factual claims if tasked directly. 
    \item Inability to handle knowledge updates: neural models rely on costly retraining to incorporate new information; this limits their utility in dynamic, real-time environments where facts are often times transient. One of the native alternatives would be to incorporate new information as part of the input through prompt engineering, but this method suffers even more from the inherent bias from the training data, as described above. 
\end{itemize}

Symbolic approaches, like knowledge graph (KG)-based reasoning, offer a more reliable and transparent alternative for fact-checking by grounding claims in structured knowledge bases like Wikidata. They allow for logic-based evaluation and break down complex claims into simpler, independently verifiable parts.
This research proposes a hybrid approach that bridges neural and symbolic methods. The pipeline included several steps: creation of an local knowledge graph, relation extraction in the form of semantic triples, claim evaluation against the knowledge graph, and response synthesis using a large language model(LLM); the end result was an automated pipeline that provides human-readable and verifiable (to the exact source) text regarding the factuality of any given claim. This novel method should solve the above potential problems by combining the scalability of neural systems and the readability and precision of symbolic reasoning. 

\section{Related Works}
It should be noted that Zhong et al. (2020)\cite{zhong} achieved high accuracy on the FEVER benchmark by leveraging semantic role labeling (SRL) for claim extraction and evidence alignment. In contrast, the pipeline of this research applied SRL to claims only, as evidence is derived directly from the Wikidata knowledge base. This ensured a more scalable and reliable fact-checking pipeline by minimizing the reliance on external text corpora and human-curated evidence.

\section{Method}
The proposed pipeline could be decomposed into several overarching steps:

\subsection{Building the Knowledge Graph: Naive Triple Extraction}
Our naive approach to knowledge graph contruction consisted of splitting each passage into sentence-sized chunks and passing each chunk to the 2a-GPT-4o-mini instance in turn. We prompted the model to extract all semantic triples (entityA, relationship, entityB) from the input text using a few-shot chain-of-thought (CoT) prompting method.

We decided on a few-shot CoT prompting strategy based on a qualitative exploratory analysis comparing zero-shot, one-shot, and few-shot promting with and without CoT. We found that few-shot CoT prompting extracted the most semantic triples from a passage and performed the most consistently compared to other prompting strategies. We also used a model temperature of $0$ to further minimize variation when sampling.

\subsection{Building the Knowledge Graph: Semantic Triplestore}
In an effort to formulate a more scalable approach to fact-checking, we considered the FEVER v1.0 Paper Development Dataset\cite{fever}. We built a highly compact, local knowledge base as follows:

We began by downloading all Wikipedia articles referenced in the FEVER dataset. The FEVER dataset contains a set of Wikipedia pages in the form of their URLs in its 'evidence\_wiki\_url' column. These URLs provide the factual basis for which their respective claims are evaluated on. A total of 1460 pages were selected for this purpose.

We split the Wikipedia articles into sentence-sized chunks with the title of the article prepended to the sentence; a total of 17186 data chunks were obtained. A naive preprocessing was performed to replace all third-person personal pronouns (he/she/they/them/it) found in all sentences with the corresponding article title. It was assumed that the vast majority of these pronouns should be referring to the same entity as the article title, and thus it was theorized that this preprocessing would make deciphering complex relations easier down the pipeline.

We then followed a similar process to the above naive triple extraction to generate a set of all claims made within each chunk. A significant benefit of this method is the triviality of updating the knowledge graph, since new knowledge can be appended to the existing graph after some light processing.

Then, for each extracted semantic triple, we performed a grounding process. We attempted to match the two entities and relationship involved in the triple on a lexical (text-match), and then semantic (embedding similarity), basis to pre-existing items in our ontology (the set of existing entities and relationships in our knowledge base). If no lexical match or semantic match with cosine similarity greater than 0.9 was found, we declared a new item in our ontology. We did so by calculating a text embedding for the item using the all-MiniLM-L6-v2 model from HuggingFace, producing 384 byte embeddings and storing them within a chromadb local vector database. In this way, we eliminated the need of postprocessing over polymorphic representations of the elements by avoiding unnecessary duplication of semantically identical but differently-phrased relationships and entities (e.g. actedIn vs actsIn).

In addition to generating embeddings for each of the three items that comprise a semantic triple, we concatenated the embeddings of entityA, relationship, and entityB to form a 'fact embedding' of dimensionality 1152 (=384*3).

\subsection{Factual Recall and Claim Evaluation}
We underwent a similar process to knowledge graph construction, extracting semantic triples via LLM and grounding said triples in our ontology. Then, we created 'claim embeddings' for each of these semantic triples via the same concatenation method used for the 'fact embeddings' above. We searched for the top $k=3$ matches in our knowledge base (cosine similarity), treating those as relevant facts for the purpose of our fact check. Finally, we passed the relevant facts and the claim to an 'arbiter' LLM (GPT-4o-mini), which was prompted to determine the validity, falsehood, or indeterminacy of the claim solely based on the relevant facts, employing a zero-shot CoT method. The arbiter's decision was then returned as the final classification in our pipeline.

This classification was made in accordance to the open-world assumption (OWA), where claims not explicitly affirmed or negated by the knowledge graph are considered undefined, and therefore no determination of factuality would be provided.

\subsection{Response Synthesis}
The fact-check results were then either 1) outputted with the relevant knowledge graph entries through a basic predetermined string (for simplicity and evaluation), or 2) fed into a LLM for a more sophisticated and human-readable result. We envision this response synthesis step as being crucial for fact-checking in socially-aligned LLMs, as it provides a more interactive user experience than simple affirmation or denial.

\section{Experiments and Analysis}
The experimental setup of this research focused on the different ways the aforementioned prompt to the LLM semantic triple extractor could be formatted, and how this difference affects the overall performance of the pipeline.

1000 passages and questions were randomly sampled from Google's BoolQ dataset (Clark et. al., 2019) to be used as the test dataset. Our process consisted of first extracting semantic triples from the passages using multi-shot chain-of-thought (CoT) prompting with the GPT-4o-mini model to form our ad hoc knowledge graph. We then passed the questions and the lexicalized knowledge graph to a GPT-4o-mini instance with zero-shot CoT prompting. This second model was prompted to determine whether the given question was true or false, based only on the provided knowledge graph.

With this method, we achieved an F-1 score of 0.86 on the binary classification task. See Table \ref{tab:result} for more results.

\begin{table}[ht]
    \centering
    \caption{Evaluation results}
    
    \caption*{Classifying Correct Answer in True/False Questions}
    \begin{tabular*}{\columnwidth}{c|c|c|c|c}
        \hline
         & precision & recall & f1-score & support \\
        \hline
        False & 0.79 & 0.89 & 0.84 & 404 \\
        True & 0.92 & 0.84 & 0.88 & 596 \\
         &  &  &  & \\
        accuracy &  &  & 0.86 & 1000 \\
        macro avg & 0.86 & 0.87 & 0.86 & 1000 \\
        weighted avg & 0.87 & 0.86 & 0.86 & 1000 \\
        \hline
    \end{tabular*}
    
    \label{tab:result}
\end{table}

It is important to note that the results in Table \ref{tab:result} were based on a miniature knowledge base constructed at test time from each passage. Ideally, the knowledge base should be based on a significantly larger dataset and not rely on the input passing in an input text. However, for our purposes, the associated API costs to do so was deemed too expensive for this research. As such, it was determined that performing our experimentation in this way was a necessary tradeoff between overall cost and still able to demonstrate the validity and effectiveness of the pipeline for the purposes of this paper. 

\section{Conclusion}
In this research, an automated pipeline for fact-checking any given text was developed and evaluated across three setups. Factual information was extracted from unstructured text data, formatted into a local knowledge graph, and used to compare and validate the factuality of other given claims. The evaluations, as shown in Table \ref{tab:result}, suggested that the pipeline was indeed able to characterize the factuality of given claims based on the knowledge graph, and did so with quality. The results also demonstrated that, despite the use of stochastic models like LLMs (which lack \emph{a priori} guarantees on correctness, relying instead on statistical patterns to elicit desired behavior), the pipeline was able to adeptly combine the adaptability of LLMs and the price logical representation of the knowledge graph to produce high-quality results. 

Because of the evident quality of the result and the expandability of the knowledge graph, it is likely that the pipeline can be deployed for practical uses after further expanding the knowledge graph to include more entries from, for example, Wikipedia. This pipeline should server as a great tool in assistance to fact-checking in general. 

\section*{Note on Diversity}
The diversity in our group stems both from our ethnic/cultural identities and our fields of study. Our group members come from India, China, and the United States. We also specialize in Physics, Hispanic Linguistics, and Business Administration in addition to our Computer Science major. Thanks to these two factors, we were able to bring three different perspectives to our project. This in turn allowed us to create a cleaner and more efficient codebase and write a high-quality, in-depth report.

\section*{Note on Individual Contributions}
Nolan built the code for the user pipeline and the knowledge graph construction. Rohan worked with Nolan on the knowledge graph construction and oversaw model evaluation. Simon worked with Rohan on the model evaluation and wrote the majority of the project report.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\end{document}
